{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is designed to take as input two files:  a FASTA file of sequences, and Query file. \n",
    "From the FASTA file, contigs are assembled, and the query file contains a sequence subsection that is\n",
    "used to query the assembled contigs, wherein the goal is to find the longest contig containing the query sequence. \n",
    "The outputs are as follows:\n",
    "ALLELES.FASTA which is a FASTA file of the largest constructed contig containing the query sequence\n",
    "ALLELES.ALN which is a tab-delimited file describing the alignment of sequence reads to the assembled contig(s) in\n",
    "the ALLELES.FASTA file. \n",
    "The columns in ALLELES.ALN are as follows: \n",
    "SSEQID name of sequencing read (from READS.fastq.gz)\n",
    "QSEQID name of contig matched (from ALLELES.fasta)\n",
    "SSTART starting coordinate in sequencing read sseqid that matches qseq\n",
    "SEND ending coordinate in sequencing read  sseqid that matches qseq\n",
    "QSTART starting coordinate in contig that matches sseq\n",
    "QEND ending coordinate in contig that matches sseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section does the following:\n",
    "Imports necessary modules and packages\n",
    "# The following two actions have defaults upon no user entry based on the location of the script.\n",
    "Takes user inputs for the directory of FASTA file to be assembled\n",
    "Takes user input for the directory of a query file \n",
    "# The following two actions have defaults upon no user entry, filenames default to those included in the repository\n",
    "Takes user input for the filename of a FASTA file to be assembled\n",
    "Takes user input for the filename of a query file to be used to find matches within the assembled contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os.path\n",
    "import bisect\n",
    "import string\n",
    "from itertools import permutations\n",
    "# import unittest\n",
    "# from unittest import mock\n",
    "# from unittest import TestCase\n",
    "# from io import StringIO\n",
    "\n",
    "# These global variables define the path in which this workflow will be run, as well as the files for both reads \n",
    "# and query inputs. \n",
    "# These inputs default to the local directory in which this notebook is stored, along with default filenames. \n",
    "# The Query file is expected to have a first line identifiying the file as query, if this in untrue remove the \"[1:]\"\n",
    "#  after readlines\" on line 29.\n",
    "\n",
    "ReadDirectory = (input(\"Enter the directory of the FASTA reads file: \")) or (os.path.join(sys.path[0]))\n",
    "QueryDirectory = (input(\"Enter the directory of the QUERY query file: \")) or (os.path.join(sys.path[0]))\n",
    "ReadFilename = (input(\"Enter the filename of the FASTA reads file: \")) or (\"READS.fasta\")\n",
    "QueryFilename = (input(\"Enter the filename of the  reads file: \")) or (\"QUERY.fasta\") \n",
    "\n",
    "#This function takes the user inputs (or default values) and stores the information read from the files \n",
    "# Secondarily for some reason the QUERY file cannot be stripped of it's various newline and brackets when read \n",
    "def Test_FASTAreader(ReadDirectory, QueryDirectory, ReadFilename, QueryFilename):\n",
    "    global FASTAreads\n",
    "    global FASTAquery\n",
    "    ReadFile = open(os.path.join(ReadDirectory, ReadFilename))\n",
    "    QueryFile = open(os.path.join(QueryDirectory, QueryFilename))\n",
    "    FASTAreads = ReadFile.read() \n",
    "    ReadFile.close\n",
    "    FASTAquery = QueryFile.readlines()[1:]\n",
    "    QueryFile.close\n",
    "# This converts the line read from the query file (after skipping the identifier) to a string\n",
    "    FASTAquery = str(FASTAquery)\n",
    "# This step removes artifacts from the query file such that no brackets, newline characters, or quotations remain.\n",
    "    FASTAquery = FASTAquery[1:-4]   \n",
    "Test_FASTAreader(ReadDirectory, QueryDirectory, ReadFilename, QueryFilename)\n",
    "\n",
    "def Test_QuerySequenceFinder(Contigs, FASTAquery):\n",
    "    QueryForward = FASTAquery\n",
    "    QueryBackward = QueryForward[::-1]\n",
    "    print(QueryForward)\n",
    "    print(QueryBackward) \n",
    "    global Name_of_variables_for_AllelesFasta\n",
    "    global Name_of_Variable_for_AlleleALN\n",
    "    Name_of_variables_for_AllelesFasta = \"The FASTA part of this is that I need to code FASTA than I am\"\n",
    "    Name_of_Variable_for_AlleleALN = \"My alignment is chaotic sad.\"\n",
    "# Test_QuerySequenceFinder(Contigs, FASTAquery)\n",
    "\n",
    "def Variables_to_files(Name_of_variables_for_AllelesFasta, Name_of_Variable_for_AlleleALN):\n",
    "    pass\n",
    "# Variables_to_files(Name_of_variables_for_AllelesFasta, Name_of_Variable_for_AlleleALN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes functions related to the creation of a graph with nodes as k-1mers and edges a kmers.\n",
    "The information stored as a list of kmers and a set of edges, and visualized via graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AAT', 'CGG', 'AAA', 'CTG', 'ACT', 'TCG', 'GAA'}\n",
      "[('AAA', 'AAC'), ('AAC', 'ACT'), ('ACT', 'CTG'), ('CTG', 'TGA'), ('TGA', 'GAA'), ('GAA', 'AAT'), ('AAT', 'ATC'), ('ATC', 'TCG'), ('TCG', 'CGG'), ('CGG', 'GGA'), ('GGA', 'GAA'), ('GAA', 'AAA'), ('AAA', 'AAT')]\n",
      "The gvmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext gvmagic\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<svg height=\"764pt\" viewBox=\"0.00 0.00 163.79 764.00\" width=\"164pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 760)\">\n<title>DeBruijn Graph</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-760 159.79,-760 159.79,4 -4,4\" stroke=\"transparent\"/>\n<!-- AAT -->\n<g class=\"node\" id=\"node1\">\n<title>AAT</title>\n<ellipse cx=\"67.9\" cy=\"-738\" fill=\"none\" rx=\"27.9\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.9\" y=\"-734.3\">AAT</text>\n</g>\n<!-- ATC -->\n<g class=\"node\" id=\"node10\">\n<title>ATC</title>\n<ellipse cx=\"123.9\" cy=\"-666\" fill=\"none\" rx=\"27.1\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123.9\" y=\"-662.3\">ATC</text>\n</g>\n<!-- AAT&#45;&gt;ATC -->\n<g class=\"edge\" id=\"edge7\">\n<title>AAT-&gt;ATC</title>\n<path d=\"M80.33,-721.46C87.65,-712.31 97.05,-700.55 105.25,-690.31\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"107.99,-692.49 111.5,-682.49 102.52,-688.11 107.99,-692.49\" stroke=\"black\"/>\n</g>\n<!-- CGG -->\n<g class=\"node\" id=\"node2\">\n<title>CGG</title>\n<ellipse cx=\"124.9\" cy=\"-522\" fill=\"none\" rx=\"29.5\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"124.9\" y=\"-518.3\">CGG</text>\n</g>\n<!-- GGA -->\n<g class=\"node\" id=\"node11\">\n<title>GGA</title>\n<ellipse cx=\"125.9\" cy=\"-450\" fill=\"none\" rx=\"29.8\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125.9\" y=\"-446.3\">GGA</text>\n</g>\n<!-- CGG&#45;&gt;GGA -->\n<g class=\"edge\" id=\"edge10\">\n<title>CGG-&gt;GGA</title>\n<path d=\"M125.14,-503.7C125.25,-495.98 125.39,-486.71 125.51,-478.11\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"129.01,-478.15 125.65,-468.1 122.01,-478.05 129.01,-478.15\" stroke=\"black\"/>\n</g>\n<!-- AAA -->\n<g class=\"node\" id=\"node3\">\n<title>AAA</title>\n<ellipse cx=\"29.9\" cy=\"-306\" fill=\"none\" rx=\"29.8\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"29.9\" y=\"-302.3\">AAA</text>\n</g>\n<!-- AAA&#45;&gt;AAT -->\n<g class=\"edge\" id=\"edge13\">\n<title>AAA-&gt;AAT</title>\n<path d=\"M29.9,-324.05C29.9,-350.71 29.9,-403.89 29.9,-449 29.9,-595 29.9,-595 29.9,-595 29.9,-636.59 45.3,-682.78 56.49,-710.86\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"53.35,-712.42 60.39,-720.34 59.83,-709.76 53.35,-712.42\" stroke=\"black\"/>\n</g>\n<!-- AAC -->\n<g class=\"node\" id=\"node8\">\n<title>AAC</title>\n<ellipse cx=\"30.9\" cy=\"-234\" fill=\"none\" rx=\"29.5\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"30.9\" y=\"-230.3\">AAC</text>\n</g>\n<!-- AAA&#45;&gt;AAC -->\n<g class=\"edge\" id=\"edge1\">\n<title>AAA-&gt;AAC</title>\n<path d=\"M30.14,-287.7C30.25,-279.98 30.39,-270.71 30.51,-262.11\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"34.01,-262.15 30.65,-252.1 27.01,-262.05 34.01,-262.15\" stroke=\"black\"/>\n</g>\n<!-- CTG -->\n<g class=\"node\" id=\"node4\">\n<title>CTG</title>\n<ellipse cx=\"30.9\" cy=\"-90\" fill=\"none\" rx=\"28.7\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"30.9\" y=\"-86.3\">CTG</text>\n</g>\n<!-- TGA -->\n<g class=\"node\" id=\"node9\">\n<title>TGA</title>\n<ellipse cx=\"58.9\" cy=\"-18\" fill=\"none\" rx=\"28.7\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"58.9\" y=\"-14.3\">TGA</text>\n</g>\n<!-- CTG&#45;&gt;TGA -->\n<g class=\"edge\" id=\"edge4\">\n<title>CTG-&gt;TGA</title>\n<path d=\"M37.53,-72.41C40.76,-64.34 44.72,-54.43 48.36,-45.35\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"51.65,-46.54 52.11,-35.96 45.15,-43.94 51.65,-46.54\" stroke=\"black\"/>\n</g>\n<!-- ACT -->\n<g class=\"node\" id=\"node5\">\n<title>ACT</title>\n<ellipse cx=\"30.9\" cy=\"-162\" fill=\"none\" rx=\"28.7\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"30.9\" y=\"-158.3\">ACT</text>\n</g>\n<!-- ACT&#45;&gt;CTG -->\n<g class=\"edge\" id=\"edge3\">\n<title>ACT-&gt;CTG</title>\n<path d=\"M30.9,-143.7C30.9,-135.98 30.9,-126.71 30.9,-118.11\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"34.4,-118.1 30.9,-108.1 27.4,-118.1 34.4,-118.1\" stroke=\"black\"/>\n</g>\n<!-- TCG -->\n<g class=\"node\" id=\"node6\">\n<title>TCG</title>\n<ellipse cx=\"124.9\" cy=\"-594\" fill=\"none\" rx=\"28.7\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"124.9\" y=\"-590.3\">TCG</text>\n</g>\n<!-- TCG&#45;&gt;CGG -->\n<g class=\"edge\" id=\"edge9\">\n<title>TCG-&gt;CGG</title>\n<path d=\"M124.9,-575.7C124.9,-567.98 124.9,-558.71 124.9,-550.11\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"128.4,-550.1 124.9,-540.1 121.4,-550.1 128.4,-550.1\" stroke=\"black\"/>\n</g>\n<!-- GAA -->\n<g class=\"node\" id=\"node7\">\n<title>GAA</title>\n<ellipse cx=\"87.9\" cy=\"-378\" fill=\"none\" rx=\"29.8\" ry=\"18\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.9\" y=\"-374.3\">GAA</text>\n</g>\n<!-- GAA&#45;&gt;AAT -->\n<g class=\"edge\" id=\"edge6\">\n<title>GAA-&gt;AAT</title>\n<path d=\"M83.95,-395.94C78.12,-422.45 67.9,-475.44 67.9,-521 67.9,-595 67.9,-595 67.9,-595 67.9,-635 67.9,-681.35 67.9,-709.92\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"64.4,-709.95 67.9,-719.95 71.4,-709.95 64.4,-709.95\" stroke=\"black\"/>\n</g>\n<!-- GAA&#45;&gt;AAA -->\n<g class=\"edge\" id=\"edge12\">\n<title>GAA-&gt;AAA</title>\n<path d=\"M75.03,-361.46C67.44,-352.31 57.7,-340.55 49.21,-330.31\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"51.81,-327.96 42.73,-322.49 46.42,-332.43 51.81,-327.96\" stroke=\"black\"/>\n</g>\n<!-- AAC&#45;&gt;ACT -->\n<g class=\"edge\" id=\"edge2\">\n<title>AAC-&gt;ACT</title>\n<path d=\"M30.9,-215.7C30.9,-207.98 30.9,-198.71 30.9,-190.11\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"34.4,-190.1 30.9,-180.1 27.4,-190.1 34.4,-190.1\" stroke=\"black\"/>\n</g>\n<!-- TGA&#45;&gt;GAA -->\n<g class=\"edge\" id=\"edge5\">\n<title>TGA-&gt;GAA</title>\n<path d=\"M64.63,-35.82C73.08,-62.18 87.9,-114.95 87.9,-161 87.9,-235 87.9,-235 87.9,-235 87.9,-275 87.9,-321.35 87.9,-349.92\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"84.4,-349.95 87.9,-359.95 91.4,-349.95 84.4,-349.95\" stroke=\"black\"/>\n</g>\n<!-- ATC&#45;&gt;TCG -->\n<g class=\"edge\" id=\"edge8\">\n<title>ATC-&gt;TCG</title>\n<path d=\"M124.14,-647.7C124.25,-639.98 124.39,-630.71 124.51,-622.11\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"128.01,-622.15 124.65,-612.1 121.01,-622.05 128.01,-622.15\" stroke=\"black\"/>\n</g>\n<!-- GGA&#45;&gt;GAA -->\n<g class=\"edge\" id=\"edge11\">\n<title>GGA-&gt;GAA</title>\n<path d=\"M117.08,-432.76C112.55,-424.4 106.91,-414.02 101.8,-404.61\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"104.77,-402.75 96.93,-395.63 98.62,-406.09 104.77,-402.75\" stroke=\"black\"/>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# De Bruijn graph creation (almost, not quite), function named with the intent to describe the phoenetics, \n",
    "# while also being mildly humorous.\n",
    "# Ideas for this function inspired by https://www.youtube.com/watch?v=f5kgmqcwb8M \n",
    "\n",
    "def DuhBrewin(txt_to_be_kmerized, k):\n",
    "    edges = []\n",
    "    nodes = set()\n",
    "    # Loop through the text to identify individual sequences\n",
    "    for seq in txt_to_be_kmerized :\n",
    "        for i in range(len(seq) - k + 1): # Loop through sequences to create kmers\n",
    "            edges.append((seq[i : i + k - 1], seq[i + 1: i + k])) # append the list of edges using the connections between k-1mers as kmers\n",
    "# Left and right k-1mers  added to edge list #\n",
    "        nodes.add(seq[i:i+k-1]) # and establish the nodes of k-1mers\n",
    "        nodes.add(seq[i+1:i+k]) # still k-1mer, just moved over one position \n",
    "    return nodes, edges #no sense in doing this without returning the values\n",
    "\n",
    "# Arbitrary sequence AAACTGAATCGGAAT\n",
    "Testing_multi_kmer = [\"AAACTG\", \"CTGAAT\", \"AATCGG\", \"CGGAAAT\"]\n",
    "\n",
    "nodes, edges = DuhBrewin(Testing_multi_kmer, 4)\n",
    "print(nodes)\n",
    "print(edges)\n",
    "\n",
    "def graph_representation(st, k):\n",
    "    # This function creates a graphical representation of the De Bruijn graph created by the DuhBrewin function, and outputs it for easier\n",
    "    # interpretation.\n",
    "    nodes, edges = DuhBrewin(st, k)  #Inputs for the nodes/edges in the graph to be ##\n",
    "    dot_str = 'digraph \"DeBruijn Graph\" {\\n' # Specifies directed graph, DeBruijn in this case #  \n",
    "    for node in nodes:\n",
    "        dot_str += ' %s [label=\"%s\"] ;\\n ' % (node, node) # Adds nodes and their respectives labels #\n",
    "    for src, dst in edges:\n",
    "        dot_str += ' %s -> %s ;\\n ' % (src, dst) # As above, no labels necessary in this case\n",
    "    return dot_str + '}\\n' \n",
    "    \n",
    "%load_ext gvmagic \n",
    "%dotstr graph_representation(Testing_multi_kmer, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the input of a list of nodes and edges, and creates an adjacency matrix with nodes the identities for both the \n",
    "# columns and rows, and matrix entries as numerical representations of the number of edges between two given sets of nodes. \n",
    "def Eulerian_walk_function(nodes, edges):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section of the notebook includes functions designed for comparison and overlapping of reads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('ACGGATGATC', 'GATCAAGT'): 4, ('TTCACGGA', 'ACGGATGATC'): 5, ('GATTCACGGA', 'ACGGATGATC'): 5, ('GATTCACGGA', 'TTCACGGA'): 8}\n"
     ]
    }
   ],
   "source": [
    "# This is a mapping function for the overlaps present in the sequences, which could be useful for \n",
    "# alignment of sequences into contigs in later functions. \n",
    "\n",
    "# When choosing values of K, consider the following: \n",
    "# Kmer size is informed by read length/depth. The value of k shoudl never exceed the length of reads. \n",
    "# Smaller k can cause issues with repeats affecting kmer occurence, which can result in multiple Eulerian paths\n",
    "# through a given graph. Larger values of k can create more issues with sequencing errors, and can result in \n",
    "# fewer connections to a given kmer node. For further reading please see link:\n",
    "# https://homolog.us/blogs/genome/2012/10/10/multi-kmer-de-bruijn-graphs/ \n",
    "\n",
    "\n",
    "def overlap(sequence_a, sequence_b, min_overlap_length=3):\n",
    "    start = 0 \n",
    "# First index start point to ensure the function begins at the leftmost/earliest character in the \n",
    "# sequence_a string. \n",
    "    while True:\n",
    "            start = sequence_a.find(sequence_b[:min_overlap_length], start) \n",
    "# Find the overlap of the suffix of sequence_b in sequence_a\n",
    "            if start == -1: #This checks for any occurences to the right of our start\n",
    "                return 0\n",
    "            if sequence_b.startswith(sequence_a[start:]):\n",
    "                return len(sequence_a) - start\n",
    "            start += 1 # Moves beyond the previous match to continue searching\n",
    "\n",
    "# Creates and stores a hashmap of the overlaps of the reads so long as they overlap by k base pairs \n",
    "def overlap_map(reads, k):\n",
    "    ovlaps_dict = {} # Dictionary where keys are pairs of reads, values are lengths of overlaps. \n",
    "    # The first member in the pair contains the prefix, the second member of the pair contains the suffix \n",
    "    for a,b, in permutations(reads, 2):\n",
    "        ovlap_length = overlap(a , b, min_overlap_length=k)\n",
    "        if ovlap_length > 0:\n",
    "            ovlaps_dict[(a, b)] = ovlap_length\n",
    "    return ovlaps_dict\n",
    "\n",
    "reads_for_olap = ['ACGGATGATC', 'GATCAAGT', 'TTCACGGA', \"GATTCACGGA\"]\n",
    "print(overlap_map(reads_for_olap, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 14]\n",
      "TCTA\n",
      "TCTA\n",
      "TCTA\n",
      "Look mom, we did it!\n"
     ]
    }
   ],
   "source": [
    "# The implementation of alignment is now focused on the alignment itself, using K-mer compositions of the query sequence and the sequence\n",
    "# to be aligned to. K-mer composition is then indexed after finding all unique K-mer compositions for both targets, and building a \n",
    "# reference hashtable for both for alignment purposes. As the goal is only the alignment of the longest contig that contains the query sequence\n",
    "# this can be sped along as there is a fair chance some reads will not be involved in the final assembled contig. \n",
    "\n",
    "# This also doubles as the alignment indexer for each read, which can be stored according to the read identity in the FASTA file which\n",
    "# is stored in a dictionary in the first steps of this program. The hashtable can then be queried using a binary search, making lookup of\n",
    "# where Kmers are located rapid, assuming of course after the initial hashtable creation the data is sorted by Kmer alphabetically, which\n",
    "# would then immediately split search space in half for every new search it conducts until the hashtable is exhausted or the kmer location\n",
    "# is found. The hashtable is built by first querying the unique K-mers of a given size for a sequence (instead of computing all possible\n",
    "# compositions of Kmers only those present in the sequence are used), following the sliding window approach of finding all unique Kmers, \n",
    "# those kmers are used to create keys for a dictionary, and while that happens the dictionary updates the value pairs for each key by\n",
    "# referencing the location that the Kmer occurs. \n",
    "\n",
    "class Index(object):\n",
    "    # Input t is text, k is the value of k to be used for kmer composition creation\n",
    "    def __init__(self, t, k):\n",
    "        self.k = k\n",
    "        self.index = []\n",
    "# Provides every index for the sequence passed in, without going beyond the length of the sequence being indexed\n",
    "        for i in range(len(t) - k +1):\n",
    "            self.index.append((t[i:i+k], i))\n",
    "        self.index.sort()\n",
    "# Without sorting binary search cannot be reasonably used for lookup, even if the sorting process may take a while it makes the exploration\n",
    "# of the search space much faster as the search space is no longer O(n), but O(log base 2 n). \n",
    "# See: https://www.rudikershaw.com/articles/whichsearch \n",
    "\n",
    "# This function takes the index list created above as input as well as a kmer designated as \"p\" for pattern, and can then query the \n",
    "# sorted list to find the matches in a given string. \n",
    "    def query(self, p):\n",
    "        kmer = p[:self.k] # This takes the pattern (p), and find the first k-bases of p to find where this pattern is in the index \n",
    "        i = bisect.bisect_left(self.index, (kmer, -1)) # This just finds the position this patterns occurs, assuming the positions is >-1\n",
    "        kmer_matches = []\n",
    "        while i < len(self.index):\n",
    "# This break is for stopping if the location in the index is not the kmer, we can stop, as we know the index is sorted, so the matches are\n",
    "# guaranteed to be exhausted after we pass the final occurence of our Kmer \n",
    "            if self.index[i][0] != kmer:\n",
    "                break\n",
    "# This is the instance of where there is a match of the first k characters of our pattern p in to the text we are querying, and then the \n",
    "# position at which this match occurs is appended to the index. \n",
    "            kmer_matches.append(self.index[i][1])\n",
    "            i += 1\n",
    "        return kmer_matches\n",
    "\n",
    "# This takes the first k units of a query pattern (p), matches it against a text to be queried (t), and then finds all the offsets of those \n",
    "# matches, records them into a list, and more importantly also checks that not only does the first k characters match the text being\n",
    "# queried, but also ensures that the remaning bases in the text being queried with pattern p match for the length of p. \n",
    "def query_Index(p, t, index):\n",
    "    k = index.k\n",
    "    offsets = []\n",
    "    for i in index.query(p):\n",
    "        if p[k:] == t[ i + k :i + len(p) ]:\n",
    "            offsets.append(i)\n",
    "    return offsets\n",
    "\n",
    "# Quick sanity check for testing, will be made into a fully testable aspect of the code later \n",
    "\n",
    "test_t_for_indexing_function = \"GCTACGATCTAGAATCTA\"\n",
    "test_p_for_indexing_function = \"TCTA\"\n",
    "\n",
    "index = Index(test_t_for_indexing_function, 2)\n",
    "\n",
    "print(query_Index(test_p_for_indexing_function, test_t_for_indexing_function, index))\n",
    "\n",
    "# One final double check to find out whether that supposed match is a complete match by printing the query string from the starting index\n",
    "# to the length of the pattern being used to search it. If they do not match, do not proceed. \n",
    "\n",
    "print(test_t_for_indexing_function[7:(7 + len(test_p_for_indexing_function))])\n",
    "Sanity_check_for_indexer = (test_t_for_indexing_function[7:(7 + len(test_p_for_indexing_function))])\n",
    "print(Sanity_check_for_indexer)\n",
    "print(test_p_for_indexing_function)\n",
    "if Sanity_check_for_indexer != test_p_for_indexing_function:\n",
    "    print(\"The indexing functtion is malfunctioning.\")\n",
    "else:\n",
    "    print(\"Look mom, we did it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the pathfinding algorithm works later and contigs are assembled, this function will take contigs and the query sequence as input\n",
    "# and as ouput produce the alignment of the query sequence to a given contig, indexing where and how it aligns (f/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boyer moore matching algorithm, inspired by ADS1: Practical: implementing Boyer-Moore on youtube, accesible at the following link:\n",
    "# https://youtu.be/CT1lQN73UMs \n",
    "# Note that this is a playlist, and has several components \n",
    "\n",
    "# Pre-processes the reads to find the positional arguments of where the text mismatches, and either a mismatch of an alignment becomes\n",
    "# a match, or the character index of the mismatch is passed. This is the \"bad character\" rule, and when combined with the \"good suffix\"\n",
    "# rule, makes alignment faster. The \"good suffix\" rule uses a subset of the string being aligned that is repeated within the full string, and\n",
    "# aligns using the next matching subset within the string. This will continue as the suffix continues to improve alignment stepwise, \n",
    "# until either the alignment fully matches or all possible alignments are exhausted, and no complete alignment is found. When combined, \n",
    "# the calculated values of shifts for either rule is used to choose which to proceed with, and whichever number of alignments skipped\n",
    "# is larger is used. Hence, many improper alignments will be skipped because they cannot be correct. This allows characters that are not\n",
    "# part of the text to be aligned to be skipped, as well as knowably incorrect alignments to be skipped. Secondarily, the number of skips\n",
    "# can be precalculated using the pattern that is being aligned. Thus, the choice of whether to use either rule can be predetermined, and \n",
    "# a lookup table can be created to further speed up the comparison process, by using a table of length n, which corresponds to the length\n",
    "# of the query string (not the string of characters it is being aligned to) by 4, the 4 being the possible nucleotides it can mismatch\n",
    "# for when it is being aligned. This could be implemented for other uses, e.g., protein alignment, and instead have an n x 20 table. \n",
    "\n",
    "## A theorem called the \"Gusfield theorem\", the principals of which explain how to do preprocessing of alignment\n",
    "## while using the Boyer-Moore algorithm, is explained in detail at the following URL: \n",
    "## http://www.cbcb.umd.edu/confcour/Spring2010/CMSC858W-materials/restrict/Gusfield-0-1-2-3.3.pdf\n",
    "## I will try to use terms described above throughout to show what each section of the code is doing, as each part is inspired by the text\n",
    "## linked above as well as the youtube linked above. \n",
    "import string\n",
    "# This is the really neato pre-processing part, where the initial values of a string to be used for alignment can be calculated for the\n",
    "# \"good suffix\" or \"bad character\" rules, using a bunch of stuff from the Gusfield theorem, which basically involved calculating a series\n",
    "# of arrays and leveraging them to make the lookup table for when to use which rule. \n",
    "\n",
    "# This approach may be less appropriate than kmer indexing and tracking, but currently that is something I do not yet know how to implement,\n",
    "# depending on how the meeting with my tutor goes I may be able to take that approach instead, which seems more efficient. \n",
    "\n",
    "# This creates the z array from a given string (s), which is used in the creation of a gang of other arrays which eventually result in a \n",
    "# table of when to implement uses of each rule based on mismatch information. \n",
    "\n",
    "\n",
    "### THIS IS NOT YET ALTERED OR FULLY COMMENTED, it is the baseline for the function to be built for preprocessing ###\n",
    "\n",
    "def z_array(s):\n",
    "\n",
    "    assert len(s) > 1\n",
    "    z = [len(s)] + [0] * (len(s)-1)\n",
    "    for i in range( 1, len(s)):\n",
    "        if s[i] == s[i-1]:\n",
    "            z[1] += 1\n",
    "        else: \n",
    "            break\n",
    "\n",
    "        r, l = 0, 0\n",
    "        if z[1] > 0:\n",
    "            r, l = z[1], 1\n",
    "\n",
    "        for k in range (2, len(s)):\n",
    "            assert z[k] ==0 \n",
    "            if k > r:\n",
    "                for i in range (k, len(s)):\n",
    "                    if s[i] == s[i-k]:\n",
    "                        z[k] += 1\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                nbeta = r - k +1\n",
    "                zkp = z[k -1 ]\n",
    "                if nbeta > zkp:\n",
    "                    z[k] = zkp\n",
    "                else: \n",
    "                    nmatch = 0\n",
    "                    for i in range (r+1, len(s)):\n",
    "                        if s[i] == s[i - k]:\n",
    "                            nmatch += 1\n",
    "                        else:\n",
    "                            break\n",
    "    return z\n",
    "\n",
    "# Compiles the N array (Gusfield Theorem) from the Z array\n",
    "\n",
    "def n_array(s):\n",
    "    return z_array(s[::1][::-1])\n",
    "\n",
    "def big_l_prime_array(p, n):\n",
    "\n",
    "    # Compile L prime array (gusfield theorem) using p and N array. \n",
    "    lp = [0] * len(p)\n",
    "    for j in range(len(p)-1):\n",
    "        i = len(p) - n[j]\n",
    "        if i < len(p):\n",
    "            lp[i] = j + 1\n",
    "    return lp\n",
    "\n",
    "def big_l_array(p, lp):\n",
    "\n",
    "# Compiles the L array described in the Gusfield theorem, using p and L prime array.\n",
    "    l = [0] * len(p)\n",
    "    l[1] = lp[1]\n",
    "    for i in range(2, len(p)):\n",
    "        l[i] = max(l[i-1], lp[i])\n",
    "    return l\n",
    "\n",
    "\n",
    "def small_l_prime_array(n):\n",
    "\n",
    "    # Compile lp' array (gusfield theorem 2.2.4) using N array\n",
    "    small_lp = [0] * len(n)\n",
    "    for i in range(len(n)):\n",
    "        if n[i] == i + 1:\n",
    "            small_lp[len(n)-i-1] = i + 1 ## this is the prefix/suffix matching aspect, which is part of the \"good suffix\" rule\n",
    "    for i in range(len(n)-2, -1, -1): \n",
    "        if small_lp[i] == 0:\n",
    "            small_lp[i] == small_lp[i + 1]\n",
    "    return small_lp\n",
    "\n",
    "def good_suffix_table(p):\n",
    "\n",
    "    # Gives us the tables that we need to know when to apply the good suffix rule #\n",
    "    n = n_array(p)\n",
    "    lp = big_l_prime_array(p, n)\n",
    "    return lp, big_l_array(p, lp), small_l_prime_array\n",
    "\n",
    "def good_suffix_mismatch(i, big_l_prime, small_l_prime):\n",
    "# Using L array, L' array, l array, and l' array, gives the amount to shift when using the good suffix rule\n",
    "    length = len(big_l_prime)\n",
    "    assert i < length -1\n",
    "    if i == length -1:\n",
    "        return 0\n",
    "    i += 1 \n",
    "    # i in this scenario is the patterns earliest match in comparison to the sequence being aligned to\n",
    "    if big_l_prime[i] > 0:\n",
    "        return length - big_l_prime[i]\n",
    "    return length - small_l_prime[i]\n",
    "\n",
    "def good_suffix_match(small_l_prime):\n",
    "# In the instance of a complete match of the query sequence to the sequence to be aligned to, output the shift calculated based on the \n",
    "# \"good suffix\" rule. \n",
    "    return len(small_l_prime) - small_l_prime[1]\n",
    "\n",
    "def dense_bad_char_tab(p, amap):\n",
    "# Using the query sequence and a list with the possible characters (sorted), create and return a table of the \"bad characters\".\n",
    "# The table is indexed by offset, then by character\n",
    "    tab = []\n",
    "    nxt = [0] * len(amap)\n",
    "    for i in range(0, len(p)):\n",
    "        c = p[i]\n",
    "        assert c in amap\n",
    "        tab.append(nxt[:])\n",
    "        nxt[amap[c]] = i+1\n",
    "    return tab \n",
    "\n",
    "class BoyerMoore(object):\n",
    "# Encapsulates pattern and associated Boyer-Moore preprocessing\n",
    "\n",
    "    def __init__(self, p, alphabet = 'ACGT'):\n",
    "        self.p = p\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "#Makes a map from the alphabet to integers\n",
    "\n",
    "        self.amap = {}\n",
    "        for i in range(len(self.alphabet)):\n",
    "            self.amap[self.alphabet[i]] = i\n",
    "        \n",
    " # Bad charcter table creation\n",
    "        self.bad_char = dense_bad_char_tab(p, self.amap)\n",
    "\n",
    "# Good suffix table creation\n",
    "        _, self.big_l, self.small_l_prime = good_suffix_table(p)\n",
    "\n",
    "    def bad_character_rule(self, i , c):\n",
    "# Outputs the the number of skips using the bad character rule at a given offset, i #\n",
    "        assert c in self.amap\n",
    "        ci = self.amap[c]\n",
    "        assert i > (self.bad_char[i][ci]-1)\n",
    "        return i - (self.bad_char[i][ci]-1)\n",
    "    \n",
    "    def good_suffix_rule(self, i):\n",
    "# Given a mismatch at offest i, return amount to shift as determed by the good suffix rule\n",
    "        length = len(self.big_l)\n",
    "        assert i < length - 1\n",
    "        if i == length -1:\n",
    "            return 0\n",
    "        i += 1 # Again, earliest match of a given query sequence to a sequence to be compared to in this case\n",
    "        if self.big_l[i] > 0:\n",
    "            return length - self.big_l[i]\n",
    "        return length -self.small_l_prime[i]\n",
    "    \n",
    "    def match_skip(self):\n",
    "# Outputs the amount to shift when there is an exact match\n",
    "        return len(self.small_l_prime) - self.small_l_prime[1]\n",
    "\n",
    "p = 'TCAA'\n",
    "p_bm = BoyerMoore(p)\n",
    "p_bm.bad_character_rule(2, 'T') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS NOT YET COMPLETE! Sincere apologies for that fact, juggling rotations, the project, and preparation for prelims as well as Day 1's and Day 3's for class has been incredibly challenging. I offer no excuses, only recognition that this needs to be completed, tests need to be implemented, and were it not for the very generous offer for resubmission it is highly likely that this would be impossible to amend. In recognition of that fact: thank you for your patience and graciousness while I try to repair my previous failures in the coming weeks. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
