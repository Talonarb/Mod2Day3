{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is designed to take as input two files:  a FASTA file of sequences, and Query file. \n",
    "From the FASTA file, contigs are assembled, and the query file contains a sequence subsection that is\n",
    "used to query the assembled contigs, wherein the goal is to find the longest contig containing the query sequence. \n",
    "The outputs are as follows:\n",
    "ALLELES.FASTA which is a FASTA file of the largest constructed contig containing the query sequence\n",
    "ALLELES.ALN which is a tab-delimited file describing the alignment of sequence reads to the assembled contig(s) in\n",
    "the ALLELES.FASTA file. \n",
    "The columns in ALLELES.ALN are as follows: \n",
    "SSEQID name of sequencing read (from READS.fastq.gz)\n",
    "QSEQID name of contig matched (from ALLELES.fasta)\n",
    "SSTART starting coordinate in sequencing read sseqid that matches qseq\n",
    "SEND ending coordinate in sequencing read  sseqid that matches qseq\n",
    "QSTART starting coordinate in contig that matches sseq\n",
    "QEND ending coordinate in contig that matches sseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section does the following:\n",
    "Imports necessary modules and packages\n",
    "# The following two actions have defaults upon no user entry based on the location of the script.\n",
    "Takes user inputs for the directory of FASTA file to be assembled\n",
    "Takes user input for the directory of a query file \n",
    "# The following two actions have defaults upon no user entry, filenames default to those included in the repository\n",
    "Takes user input for the filename of a FASTA file to be assembled\n",
    "Takes user input for the filename of a query file to be used to find matches within the assembled contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os.path\n",
    "import bisect\n",
    "import string\n",
    "from itertools import permutations\n",
    "global circuit \n",
    "kmer_size = 4\n",
    "import random\n",
    "random.seed = 13\n",
    "\n",
    "# Arbitrary sequence AAACTGAATCGGAAT\n",
    "# Testing_multi_kmer = [\"AAACTGCTGAAT\", \"AATCGGCGGAAAT\", \"FWENWF\"]\n",
    "Testing_multi_kmer = [\"AAACTG\", \"CTGAAT\", \"AATCGG\", \"CGGAAAT\"]\n",
    "\n",
    "# import unittest\n",
    "# from unittest import mock\n",
    "# from unittest import TestCase\n",
    "# from io import StringIO\n",
    "\n",
    "#This function takes the user inputs (or default values) and stores the information read from the files \n",
    "# Secondarily for some reason the QUERY file cannot be stripped of it's various newline and brackets when read \n",
    "def Test_FASTAreader(ReadDirectory, QueryDirectory, ReadFilename, QueryFilename):\n",
    "    global FASTAreads\n",
    "    global FASTAquery\n",
    "    ReadFile = open(os.path.join(ReadDirectory, ReadFilename))\n",
    "    QueryFile = open(os.path.join(QueryDirectory, QueryFilename))\n",
    "    FASTAreads = ReadFile.read()\n",
    "    ReadFile.close\n",
    "    FASTAquery = QueryFile.readlines()[1:]\n",
    "    QueryFile.close\n",
    "# This converts the line read from the query file (after skipping the identifier) to a string\n",
    "    FASTAquery = str(FASTAquery)\n",
    "# This step removes artifacts from the query file such that no brackets, newline characters, or quotations remain.\n",
    "    FASTAquery = FASTAquery[2:-4]   \n",
    "\n",
    "# Modified FASTA to dict function, via A.J. Uppal's code available at :\n",
    "# https://stackoverflow.com/questions/29333077/reading-a-fasta-file-format-into-python-dictionary \n",
    "def Test_FASTA_to_dict(fastafile):\n",
    "    global fasta_dict\n",
    "    fasta_dict = {}\n",
    "    with open(fastafile) as file_one:\n",
    "        for line in file_one:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                active_sequence_name = line[1:]\n",
    "                if active_sequence_name not in fasta_dict:\n",
    "                    fasta_dict[active_sequence_name] = []\n",
    "                continue\n",
    "            sequence = line\n",
    "            fasta_dict[active_sequence_name].append(sequence)\n",
    "\n",
    "def fasta_dict_to_kv_lists(dictionary):\n",
    "    global sequence_names_from_fasta\n",
    "    global sequences_from_fasta\n",
    "    sequence_names_from_fasta = list(dictionary.keys())\n",
    "    sequences_from_fasta = list(dictionary.values())\n",
    "\n",
    "def Test_QuerySequenceFinder(Contigs, FASTAquery):\n",
    "    QueryForward = FASTAquery\n",
    "    QueryBackward = QueryForward[::-1]\n",
    "    print(QueryForward)\n",
    "    print(QueryBackward) \n",
    "    global Name_of_variables_for_AllelesFasta\n",
    "    global Name_of_Variable_for_AlleleALN\n",
    "    Name_of_variables_for_AllelesFasta = \"The FASTA part of this is that I need to code FASTA than I am\"\n",
    "    Name_of_Variable_for_AlleleALN = \"My alignment is chaotic sad.\"\n",
    "# Test_QuerySequenceFinder(Contigs, FASTAquery)\n",
    "\n",
    "def Variables_to_files(Name_of_variables_for_AllelesFasta, Name_of_Variable_for_AlleleALN):\n",
    "    pass\n",
    "# Variables_to_files(Name_of_variables_for_AllelesFasta, Name_of_Variable_for_AlleleALN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement timeit function for the complex parts (not file readin, more like kmer creation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes functions related to the creation of a graph with nodes as k-1mers and edges a kmers.\n",
    "The information stored as a list of kmers and a set of edges, and visualized via graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De Bruijn graph creation (almost, not quite), function named with the intent to describe the phoenetics, \n",
    "# while also being mildly humorous.\n",
    "# Ideas for this function inspired by https://www.youtube.com/watch?v=f5kgmqcwb8M \n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "# Set vs list comprehension, is a list more approriate after using remove duplicates? Time this. \n",
    "def DuhBrewin(txt_to_be_kmerized, k):\n",
    "    edges = []\n",
    "    nodes = set()\n",
    "# Loop through the text to identify individual sequences\n",
    "    for seq in txt_to_be_kmerized :\n",
    "        for i in range(len(seq) - k + 1): # Loop through sequences to create kmers\n",
    "            edges.append((seq[i : i + k - 1], seq[i + 1: i + k])) # append the list of edges using the connections between k-1mers as kmers\n",
    "# Left and right k-1mers  added to edge list #\n",
    "        nodes.add(seq[i:i+k-1]) # and establish the nodes of k-1mers\n",
    "        print(nodes)\n",
    "        nodes.add(seq[i+1:i+k]) # still k-1mer, just moved over one position \n",
    "        print(nodes)\n",
    "    edges.sort\n",
    "    return nodes, edges #no sense in doing this without returning the values\n",
    "\n",
    "def graph_representation(st, k):\n",
    "    # This function creates a graphical representation of the De Bruijn graph created by the DuhBrewin function, and outputs it for easier\n",
    "    # interpretation.\n",
    "    nodes, edges = DuhBrewin(st, k)  #Inputs for the nodes/edges in the graph to be ##\n",
    "    dot_str = 'digraph \"DeBruijn Graph\" {\\n' # Specifies directed graph, DeBruijn in this case #  \n",
    "    for node in nodes:\n",
    "        print(node)\n",
    "        dot_str += ' %s [label=\"%s\"] ;\\n ' % (node, node) # Adds nodes and their respectives labels #\n",
    "    for src, dst in edges:\n",
    "        dot_str += ' %s -> %s ;\\n ' % (src, dst) # As above, no labels necessary in this case\n",
    "    return dot_str + '}\\n' \n",
    "    \n",
    "def rundot(s):\n",
    "    \"\"\"Execute dot and return a raw SVG image, or None.\"\"\"\n",
    "    dot = Popen(['dot', '-Tsvg'], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    stdoutdata, stderrdata = dot.communicate(s.encode('utf-8'))\n",
    "    status = dot.wait()\n",
    "    if status == 0:\n",
    "        return stdoutdata\n",
    "    else:\n",
    "        fstr = \"dot returned {}\\n[==== stderr ====]\\n{}\"\n",
    "        error(fstr.format(status, stderrdata.decode('utf-8')))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete later \n",
    "#Real_sequences_for_testing = sequences_from_fasta[0:1]\n",
    "#fixed_seqs = [item for elem in Real_sequences_for_testing for item in elem]\n",
    "#real_nodes, real_edges = DuhBrewin(fixed_seqs, 4)\n",
    "#print()\n",
    "#print(real_edges)\n",
    "#print()\n",
    "#print(fixed_seqs)\n",
    "# for 101 sequences the ~ time on (specify computer specs/processor) to create K-mers of size 35 was 1.7s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge_dict is a generated list of edges from the edge list of the kmerizaiton function\n",
    "# I want to call the dictionary generation based on the input to the pathfinding algorithm, and return a dictionary for use within\n",
    "# the pathfinding function.\n",
    "# Convert list of edges from DeBruijn function into a dictionary of source node as key and value pairs as directed targets of nodes\n",
    "def pairs_to_dict(pair_list, dictionary_to_be_made): \n",
    "    for a, b in pair_list: # loops through list of pairs \n",
    "        dictionary_to_be_made.setdefault(a, []).append(b) # makes a key of the \"a\" position in pairs with value \"b\" \n",
    "    return dictionary_to_be_made # and now we have our dictionary\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that takes the input of an edge dictionary and creates an adjacency matrix. No longer used, but useful code. \n",
    "def dict_to_matrix(edge_dict):\n",
    "    global edge_matrix\n",
    "    keys=sorted(edge_dict.keys())\n",
    "    size=len(keys)\n",
    "    edge_matrix = [ [0]*size for i in range(size) ]\n",
    "    print(edge_dict.keys())\n",
    "    print(size)\n",
    "    for a,b in [(keys.index(a), keys.index(b)) for a, row in edge_dict.items() for b in row]:\n",
    "        print(a, b)\n",
    "        edge_matrix[a][b] = 2 if (a==b) else 1\n",
    "    print(edge_matrix)\n",
    "    print(keys)\n",
    "    for i in range(size): \n",
    "        for j in range(size): \n",
    "            print(edge_matrix[i][j], end = \" \")\n",
    "        print() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Eulerian circuit in given directed graph using Hierholzer algorithm\n",
    "# Arbitrary sequence AAACTGAATCGGAAT was the input to the kmer composition/graph building part, lets see if it can get reassembled. \n",
    "\n",
    "def printCircuit(adj, starting_node):\n",
    "   global circuit_for_fncs\n",
    "   switch_for_change_start = 0\n",
    "    # adj represents the adjacency list of\n",
    "    # the directed graph\n",
    "   if len(adj) == 0:\n",
    "        return(1)\n",
    "   \n",
    "# Maintain a stack to keep vertices\n",
    "# We can start from any vertex, here we start with a given value. \n",
    "   curr_path = [starting_node]\n",
    "   #print(\"This is the function's call of starting location. It should change.\")\n",
    "   #print(starting_node)\n",
    "# list to store final circuit\n",
    "   circuit = []\n",
    "   \n",
    "   while curr_path:\n",
    "   \n",
    "        curr_v = curr_path[-1]\n",
    "           \n",
    "# If there's remaining edge in adjacency list of the current vertex \n",
    "        if adj[curr_v]:\n",
    "  \n",
    "# Find and remove the next vertex that is adjacent to the current vertex\n",
    "            next_v = adj[curr_v].pop()\n",
    "# Push the new vertex to the stack\n",
    "            curr_path.append(next_v)\n",
    "# back-track to find remaining circuit, pop function of a recursive element \n",
    "        else:\n",
    "# Remove the current vertex and put it in the circuit\n",
    "            circuit.append(curr_path.pop())\n",
    "                # switch_for_change_start = 1\n",
    "                # return(switch_for_change_start) \n",
    "# we've got the circuit, now print it in reverse\n",
    "   reversed_circuit = circuit[::-1]\n",
    "   print(reversed_circuit)\n",
    "   circuit_for_fncs = reversed_circuit\n",
    "   print(circuit_for_fncs)\n",
    "   for i in range(len(circuit) - 1, -1, -1):\n",
    "    print(circuit[i], end = \"\")\n",
    "    if i:\n",
    "        print(\" -> \", end = \"\")\n",
    "    ### This is the end of the function\n",
    "       \n",
    "\n",
    "def visualize_Eulerian_tour(edges, edge_dict):\n",
    "# Initialize a list of empty \"[]\", which act as a node, outgoing connections symbolized by any integers between the brackets, index of the \n",
    "# closed brackets corresponding to the node list \n",
    "    node_list = []\n",
    "# As the node list generated from kmerization is not necessarily complete, fill the nodes from the edge list, which will always have\n",
    "# every node visited in at least pair as either source or destination. No need for redudant nodes, but want list traversal, so using set\n",
    "# followed by list converts the list to only unique values, and from there is converted back into a list. \n",
    "    for src, dest in edges:\n",
    "        node_list.append(src)\n",
    "        node_list.append(dest)\n",
    "    print(\"This is the set of nodes generated before removing duplicates\")\n",
    "    print()\n",
    "    print(node_list)\n",
    "    # Remove duplicates vs set function, is it faster? Time it. \n",
    "    node_list = list(set(node_list))\n",
    "  \n",
    "    print(\"This is the set of nodes, in list form\")\n",
    "   \n",
    "    circuit_for_test = [ [] for _ in node_list ]\n",
    "    print('This is the list of empty nodes to be filled with integers', circuit_for_test)\n",
    "    starting = 0\n",
    "    print('This is the node list', node_list)\n",
    "    # Loop through all the nodes in the node list, adding edges based on a dictionary of outgoing connections\n",
    "    print(\"Before for\" )\n",
    "    print(len(node_list))\n",
    "    for i in node_list:\n",
    "        print(i)\n",
    "    # Raise an exception for when a node has no outgoing connections, such that starting node can be set to a different node\n",
    "        try:\n",
    "            index_to_append = int(node_list.index(i))\n",
    "            for j in edge_dict[i]:\n",
    "    # CNCT in this case is the connection \n",
    "                cnct = int(node_list.index(j))\n",
    "                circuit_for_test[index_to_append].append(cnct)\n",
    "            printCircuit(circuit_for_test, starting)\n",
    "        except KeyError: \n",
    "                print(\"There are no outgoing links from this node.\")\n",
    "    print(\"finish\")\n",
    "\n",
    "    printCircuit(circuit_for_test, starting) \n",
    "\n",
    "    # Currently using an adjacency hash to assemble an adjacency list, which is admittedly an awful and overly complicated way to solve. \n",
    "    # However, as iterating through the hash to travel edges is beyond my ability, this hacky change to adapt hash -> list -> index -> contig\n",
    "    # should still net a suitable answer, albeit more slowly and with more resources. \n",
    "\n",
    "    # Want a print function to print the value of the corresponding node in the circuit list for the first node, followed by the terminal\n",
    "    # characters of the next nodes.  \n",
    "    start_node = node_list[(circuit_for_fncs[0])]\n",
    "    start_node = str(start_node)\n",
    "    nodes_traversed = circuit_for_fncs[1::]\n",
    "    list_of_nodes_to_be_converted = []\n",
    "    print()\n",
    "    for i in circuit_for_fncs:\n",
    "        print(node_list[i] + \"->\", end = '') \n",
    "    for i in circuit_for_fncs[1:]:\n",
    "        list_of_nodes_to_be_converted.append(node_list[i])\n",
    "    list_of_converted_nodes = [x[-1] for x in list_of_nodes_to_be_converted]\n",
    "    list_of_converted_nodes_to_string = ''.join(map(str, list_of_converted_nodes))\n",
    "    contig_from_path = start_node + list_of_converted_nodes_to_string\n",
    "    print(contig_from_path)\n",
    "    return(contig_from_path)\n",
    "    \n",
    "\n",
    "    # Here we can see that although the kmer composition was correct, the path was Eulerian, and the concatenation of sequences worked, the \n",
    "    # resulting contig was not representative of the input. AAACTGAATCGGAAT =/= AATCGGAAACTGAATT, however we can see where the repeated AA \n",
    "    # sequences created issues with re-alignment. Repetitive elements aside, largely succesful. \n",
    "\n",
    "    # ADD TO LIMITATIONS THE INABILITY TO CHANGE START NODES, or ASSUME DENSELY CONNECTED GRAPHS (maybe add it as both?)\n",
    "# visualize_Eulerian_tour(edges)\n",
    "# visualize_Eulerian_tour(real_edges)\n",
    "# change the timing, get the walk looping correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section of the notebook includes functions designed for comparison and overlapping of reads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a mapping function for the overlaps present in the sequences, which could be useful for \n",
    "# alignment of sequences into contigs in later functions. \n",
    "\n",
    "# When choosing values of K, consider the following: \n",
    "# Kmer size is informed by read length/depth. The value of k should never exceed the length of reads. \n",
    "# Smaller k can cause issues with repeats affecting kmer occurence, which can result in multiple Eulerian paths\n",
    "# through a given graph. Larger values of k can create more issues with sequencing errors, and can result in \n",
    "# fewer connections to a given kmer node. For further reading please see link:\n",
    "# https://homolog.us/blogs/genome/2012/10/10/multi-kmer-de-bruijn-graphs/ \n",
    "\n",
    "\n",
    "def overlap(sequence_a, sequence_b, min_overlap_length=3):\n",
    "    start = 0 \n",
    "# First index start point to ensure the function begins at the leftmost/earliest character in the \n",
    "# sequence_a string. \n",
    "    while True:\n",
    "            start = sequence_a.find(sequence_b[:min_overlap_length], start) \n",
    "# Find the overlap of the suffix of sequence_b in sequence_a\n",
    "            if start == -1: #This checks for any occurences to the right of our start\n",
    "                return 0\n",
    "            if sequence_b.startswith(sequence_a[start:]):\n",
    "                return len(sequence_a) - start\n",
    "            start += 1 # Moves beyond the previous match to continue searching\n",
    "\n",
    "# Creates and stores a hashmap of the overlaps of the reads so long as they overlap by k base pairs \n",
    "def overlap_map(reads, k):\n",
    "    ovlaps_dict = {} # Dictionary where keys are pairs of reads, values are lengths of overlaps. \n",
    "    # The first member in the pair contains the prefix, the second member of the pair contains the suffix \n",
    "    for a,b, in permutations(reads, 2):\n",
    "        ovlap_length = overlap(a , b, min_overlap_length=k)\n",
    "        if ovlap_length > 0:\n",
    "            ovlaps_dict[(a, b)] = ovlap_length\n",
    "    return ovlaps_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation of alignment is now focused on the alignment itself, using K-mer compositions of the query sequence and the sequence\n",
    "# to be aligned to. K-mer composition is then indexed after finding all unique K-mer compositions for both targets, and building a \n",
    "# reference hashtable for both for alignment purposes. As the goal is only the alignment of the longest contig that contains the query sequence\n",
    "# this can be sped along as there is a fair chance some reads will not be involved in the final assembled contig. \n",
    "\n",
    "# This also doubles as the alignment indexer for each read, which can be stored according to the read identity in the FASTA file which\n",
    "# is stored in a dictionary in the first steps of this program. The hashtable can then be queried using a binary search, making lookup of\n",
    "# where Kmers are located rapid, assuming of course after the initial hashtable creation the data is sorted by Kmer alphabetically, which\n",
    "# would then immediately split search space in half for every new search it conducts until the hashtable is exhausted or the kmer location\n",
    "# is found. The hashtable is built by first querying the unique K-mers of a given size for a sequence (instead of computing all possible\n",
    "# compositions of Kmers only those present in the sequence are used), following the sliding window approach of finding all unique Kmers, \n",
    "# those kmers are used to create keys for a dictionary, and while that happens the dictionary updates the value pairs for each key by\n",
    "# referencing the location that the Kmer occurs. \n",
    "\n",
    "# This assumes that the kmers are more or less unique to a given sequence. Could be improved. This would not be good for a highly \n",
    "# repetitive sequence. Set assumptions in: readme and report. \n",
    "\n",
    "class Index(object):\n",
    "    # Input t is text, k is the value of k to be used for kmer composition creation\n",
    "    def __init__(self, t, k):\n",
    "        self.k = k\n",
    "        self.index = []\n",
    "# Provides every index for the sequence passed in, without going beyond the length of the sequence being indexed\n",
    "        for i in range(len(t) - k +1):\n",
    "            self.index.append((t[i:i+k], i))\n",
    "        self.index.sort()\n",
    "# Without sorting binary search cannot be reasonably used for lookup, even if the sorting process may take a while it makes the exploration\n",
    "# of the search space much faster as the search space is no longer O(n), but O(log base 2 n). \n",
    "# See: https://www.rudikershaw.com/articles/whichsearch \n",
    "\n",
    "# This function takes the index list created above as input as well as a kmer designated as \"p\" for pattern, and can then query the \n",
    "# sorted list to find the matches in a given string. \n",
    "    def query(self, p):\n",
    "        kmer = p[:self.k] # This takes the pattern (p), and find the first k-bases of p to find where this pattern is in the index \n",
    "        i = bisect.bisect_left(self.index, (kmer, -1)) # This just finds the position this patterns occurs, assuming the positions is >-1\n",
    "        kmer_matches = []\n",
    "        while i < len(self.index):\n",
    "# This break is for stopping if the location in the index is not the kmer, we can stop, as we know the index is sorted, so the matches are\n",
    "# guaranteed to be exhausted after we pass the final occurence of our Kmer \n",
    "            if self.index[i][0] != kmer:\n",
    "                return kmer_matches\n",
    "# break is bad in programs, if breaks the program, it just stops everything. Use return in lieu of break. Don't do this in prelims. \n",
    "\n",
    "# This is the instance of where there is a match of the first k characters of our pattern p in to the text we are querying, and then the \n",
    "# position at which this match occurs is appended to the index. \n",
    "            kmer_matches.append(self.index[i][1])\n",
    "            i += 1\n",
    "        return kmer_matches\n",
    "\n",
    "# This takes the first k units of a query pattern (p), matches it against a text to be queried (t), and then finds all the offsets of those \n",
    "# matches, records them into a list, and more importantly also checks that not only does the first k characters match the text being\n",
    "# queried, but also ensures that the remaning bases in the text being queried with pattern p match for the length of p. \n",
    "def query_Index(p, t, index):\n",
    "    k = index.k\n",
    "    offsets = []\n",
    "    for i in index.query(p):\n",
    "        if p[k:] == t[ i + k :i + len(p) ]:\n",
    "            offsets.append(i)\n",
    "    return offsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the pathfinding algorithm works later and contigs are assembled, this function will take contigs and the query sequence as input\n",
    "# and as ouput produce the alignment of the query sequence to a given contig, indexing where and how it aligns (f/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boyer moore matching algorithm, inspired by ADS1: Practical: implementing Boyer-Moore on youtube, accesible at the following link:\n",
    "# https://youtu.be/CT1lQN73UMs \n",
    "# Note that this is a playlist, and has several components \n",
    "\n",
    "# Pre-processes the reads to find the positional arguments of where the text mismatches, and either a mismatch of an alignment becomes\n",
    "# a match, or the character index of the mismatch is passed. This is the \"bad character\" rule, and when combined with the \"good suffix\"\n",
    "# rule, makes alignment faster. The \"good suffix\" rule uses a subset of the string being aligned that is repeated within the full string, and\n",
    "# aligns using the next matching subset within the string. This will continue as the suffix continues to improve alignment stepwise, \n",
    "# until either the alignment fully matches or all possible alignments are exhausted, and no complete alignment is found. When combined, \n",
    "# the calculated values of shifts for either rule is used to choose which to proceed with, and whichever number of alignments skipped\n",
    "# is larger is used. Hence, many improper alignments will be skipped because they cannot be correct. This allows characters that are not\n",
    "# part of the text to be aligned to be skipped, as well as knowably incorrect alignments to be skipped. Secondarily, the number of skips\n",
    "# can be precalculated using the pattern that is being aligned. Thus, the choice of whether to use either rule can be predetermined, and \n",
    "# a lookup table can be created to further speed up the comparison process, by using a table of length n, which corresponds to the length\n",
    "# of the query string (not the string of characters it is being aligned to) by 4, the 4 being the possible nucleotides it can mismatch\n",
    "# for when it is being aligned. This could be implemented for other uses, e.g., protein alignment, and instead have an n x 20 table. \n",
    "\n",
    "## A theorem called the \"Gusfield theorem\", the principals of which explain how to do preprocessing of alignment\n",
    "## while using the Boyer-Moore algorithm, is explained in detail at the following URL: \n",
    "## http://www.cbcb.umd.edu/confcour/Spring2010/CMSC858W-materials/restrict/Gusfield-0-1-2-3.3.pdf\n",
    "## I will try to use terms described above throughout to show what each section of the code is doing, as each part is inspired by the text\n",
    "## linked above as well as the youtube linked above. \n",
    "import string\n",
    "# This is the really neato pre-processing part, where the initial values of a string to be used for alignment can be calculated for the\n",
    "# \"good suffix\" or \"bad character\" rules, using a bunch of stuff from the Gusfield theorem, which basically involved calculating a series\n",
    "# of arrays and leveraging them to make the lookup table for when to use which rule. \n",
    "\n",
    "# This approach may be less appropriate than kmer indexing and tracking, but currently that is something I do not yet know how to implement,\n",
    "# depending on how the meeting with my tutor goes I may be able to take that approach instead, which seems more efficient. \n",
    "\n",
    "# This creates the z array from a given string (s), which is used in the creation of a gang of other arrays which eventually result in a \n",
    "# table of when to implement uses of each rule based on mismatch information. \n",
    "\n",
    "\n",
    "### THIS IS NOT YET ALTERED OR FULLY COMMENTED, it is the baseline for the function to be built for preprocessing ###\n",
    "\n",
    "def z_array(s):\n",
    "\n",
    "    assert len(s) > 1\n",
    "    z = [len(s)] + [0] * (len(s)-1)\n",
    "    for i in range( 1, len(s)):\n",
    "        if s[i] == s[i-1]:\n",
    "            z[1] += 1\n",
    "        else: \n",
    "            break\n",
    "\n",
    "        r, l = 0, 0\n",
    "        if z[1] > 0:\n",
    "            r, l = z[1], 1\n",
    "\n",
    "        for k in range (2, len(s)):\n",
    "            assert z[k] ==0 \n",
    "            if k > r:\n",
    "                for i in range (k, len(s)):\n",
    "                    if s[i] == s[i-k]:\n",
    "                        z[k] += 1\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                nbeta = r - k + 1\n",
    "                zkp = z[k -1 ]\n",
    "                if nbeta > zkp:\n",
    "                    z[k] = zkp\n",
    "                else: \n",
    "                    nmatch = 0\n",
    "                    for i in range (r+1, len(s)):\n",
    "                        if s[i] == s[i - k]:\n",
    "                            nmatch += 1\n",
    "                        else:\n",
    "                            break\n",
    "            l, r = k, r + nmatch\n",
    "            z[k] = r - k + 1\n",
    "    return z\n",
    "\n",
    "# Compiles the N array (Gusfield Theorem) from the Z array\n",
    "\n",
    "def n_array(s):\n",
    "    return z_array(s[::-1][::-1])\n",
    "\n",
    "def big_l_prime_array(p, n):\n",
    "\n",
    "    # Compile L prime array (gusfield theorem) using p and N array. \n",
    "    lp = [0] * len(p)\n",
    "    for j in range(len(p)-1):\n",
    "        i = len(p) - n[j]\n",
    "        if i < len(p):\n",
    "            lp[i] = j + 1\n",
    "    return lp\n",
    "\n",
    "def big_l_array(p, lp):\n",
    "\n",
    "# Compiles the L array described in the Gusfield theorem, using p and L prime array.\n",
    "    l = [0] * len(p)\n",
    "    l[1] = lp[1]\n",
    "    for i in range(2, len(p)):\n",
    "        l[i] = max(l[i-1], lp[i])\n",
    "    return l\n",
    "\n",
    "\n",
    "def small_l_prime_array(n):\n",
    "\n",
    "    # Compile lp' array (gusfield theorem 2.2.4) using N array\n",
    "    small_lp = [0] * len(n)\n",
    "    for i in range(len(n)):\n",
    "        if n[i] == i + 1:\n",
    "            small_lp[len(n)-i-1] = i + 1 ## this is the prefix/suffix matching aspect, which is part of the \"good suffix\" rule\n",
    "    for i in range(len(n)-2, -1, -1): \n",
    "        if small_lp[i] == 0:\n",
    "            small_lp[i] == small_lp[i + 1]\n",
    "    return small_lp\n",
    "\n",
    "def good_suffix_table(p):\n",
    "\n",
    "    # Gives us the tables that we need to know when to apply the good suffix rule #\n",
    "    n = n_array(p)\n",
    "    lp = big_l_prime_array(p, n)\n",
    "    return lp, big_l_array(p, lp), small_l_prime_array(n)\n",
    "\n",
    "def good_suffix_mismatch(i, big_l_prime, small_l_prime):\n",
    "# Using L array, L' array, l array, and l' array, gives the amount to shift when using the good suffix rule\n",
    "    length = len(big_l_prime)\n",
    "    assert i < length\n",
    "    if i == length -1:\n",
    "        return 0\n",
    "    i += 1 \n",
    "    # i in this scenario is the patterns earliest match in comparison to the sequence being aligned to\n",
    "    if big_l_prime[i] > 0:\n",
    "        return length - big_l_prime[i]\n",
    "    return length - small_l_prime[i]\n",
    "\n",
    "def good_suffix_match(small_l_prime):\n",
    "# In the instance of a complete match of the query sequence to the sequence to be aligned to, output the shift calculated based on the \n",
    "# \"good suffix\" rule. \n",
    "    return len(small_l_prime) - small_l_prime[1]\n",
    "\n",
    "def dense_bad_char_tab(p, amap):\n",
    "# Using the query sequence and a list with the possible characters (sorted), create and return a table of the \"bad characters\".\n",
    "# The table is indexed by offset, then by character\n",
    "    tab = []\n",
    "    nxt = [0] * len(amap)\n",
    "    for i in range(0, len(p)):\n",
    "        c = p[i]\n",
    "        assert c in amap\n",
    "        tab.append(nxt[:])\n",
    "        nxt[amap[c]] = i+1\n",
    "    return tab \n",
    "\n",
    "class BoyerMoore(object):\n",
    "# Encapsulates pattern and associated Boyer-Moore preprocessing\n",
    "\n",
    "    def __init__(self, p, alphabet = 'ACGT'):\n",
    "        self.p = p\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "#Makes a map from the alphabet to integers\n",
    "\n",
    "        self.amap = {}\n",
    "        for i in range(len(self.alphabet)):\n",
    "            self.amap[self.alphabet[i]] = i\n",
    "        \n",
    " # Bad character table creation\n",
    "        self.bad_char = dense_bad_char_tab(p, self.amap)\n",
    "\n",
    "# Good suffix table creation\n",
    "        _, self.big_l, self.small_l_prime = good_suffix_table(p)\n",
    "\n",
    "    def bad_character_rule(self, i , c):\n",
    "# Outputs the the number of skips using the bad character rule at a given offset, i #\n",
    "        assert c in self.amap\n",
    "        ci = self.amap[c]\n",
    "        assert i > (self.bad_char[i][ci]-1)\n",
    "        return i - (self.bad_char[i][ci]-1)\n",
    "    \n",
    "    def good_suffix_rule(self, i):\n",
    "# Given a mismatch at offest i, return amount to shift as determed by the good suffix rule\n",
    "        length = len(self.big_l)\n",
    "        assert i < length\n",
    "        if i == length -1:\n",
    "            return 0\n",
    "        i += 1 # Again, earliest match of a given query sequence to a sequence to be compared to in this case\n",
    "        if self.big_l[i] > 0:\n",
    "            return length - self.big_l[i]\n",
    "        return length -self.small_l_prime[i]\n",
    "    \n",
    "    def match_skip(self):\n",
    "# Outputs the amount to shift when there is an exact match\n",
    "        return len(self.small_l_prime) - self.small_l_prime[1]\n",
    "\n",
    "\n",
    "# This function does pattern matching based on bad character alignments or good suffix alignments, with arguments of the pattern, the\n",
    "# pattern's precalculated boyermoore skip rule sizes, and t, being text to compare the pattern to. \n",
    "def boyer_moore(pattern, p_bm ,t):\n",
    "    i = 0\n",
    "    occurrences = []\n",
    "    while i < len(t) - len(pattern) +1 :\n",
    "        shift = 1\n",
    "        mismatched = False\n",
    "        for j in range(len(pattern)-1, -1, -1):\n",
    "            if not pattern[j] == t[i+j]:\n",
    "                skip_bc = p_bm.bad_character_rule(j, t[i+j])\n",
    "                skip_gs = p_bm.good_suffix_rule(j)\n",
    "                shift = max(shift, skip_bc, skip_gs)\n",
    "                mismatched = True\n",
    "                break\n",
    "        if not mismatched:\n",
    "            occurrences.append(i)\n",
    "            skip_gs = p_bm.match_skip()\n",
    "            shift = max(shift,skip_gs)\n",
    "        i += shift \n",
    "    return(occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS NOT YET COMPLETE! Sincere apologies for that fact, juggling rotations, the project, and preparation for prelims as well as Day 1's and Day 3's for class has been incredibly challenging. I offer no excuses, only recognition that this needs to be completed, tests need to be implemented, and were it not for the very generous offer for resubmission it is highly likely that this would be impossible to amend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACT'}\n",
      "{'CTG', 'ACT'}\n",
      "{'GAA', 'CTG', 'ACT'}\n",
      "{'GAA', 'CTG', 'ACT', 'AAT'}\n",
      "{'CTG', 'AAT', 'ACT', 'GAA', 'TCG'}\n",
      "{'CGG', 'CTG', 'AAT', 'ACT', 'GAA', 'TCG'}\n",
      "{'AAA', 'CGG', 'CTG', 'AAT', 'ACT', 'GAA', 'TCG'}\n",
      "{'AAA', 'CGG', 'CTG', 'AAT', 'ACT', 'GAA', 'TCG'}\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gvmagic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Talon\\Downloads\\Module2Day3\\Mod3Day3Testing3.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=40'>41</a>\u001b[0m     \u001b[39mprint\u001b[39m(p)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=41'>42</a>\u001b[0m     \u001b[39m# print(dotfiledbgraph)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=42'>43</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=43'>44</a>\u001b[0m     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=65'>66</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=66'>67</a>\u001b[0m \u001b[39m# Callin main function, which calls all the other functions, the driver of the program. \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=67'>68</a>\u001b[0m main()\n",
      "\u001b[1;32mc:\\Users\\Talon\\Downloads\\Module2Day3\\Mod3Day3Testing3.ipynb Cell 17'\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=19'>20</a>\u001b[0m nodes, edges \u001b[39m=\u001b[39m DuhBrewin(Testing_multi_kmer, kmer_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=20'>21</a>\u001b[0m \u001b[39m# Use graphviz to \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=21'>22</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mload_ext\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgvmagic\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=22'>23</a>\u001b[0m dotfiledbgraph \u001b[39m=\u001b[39m rundot(graph_representation(Testing_multi_kmer, kmer_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Talon/Downloads/Module2Day3/Mod3Day3Testing3.ipynb#ch0000016?line=23'>24</a>\u001b[0m Real_sequences_for_testing \u001b[39m=\u001b[39m sequences_from_fasta[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:2264\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/interactiveshell.py?line=2261'>2262</a>\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/interactiveshell.py?line=2262'>2263</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/interactiveshell.py?line=2263'>2264</a>\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/interactiveshell.py?line=2264'>2265</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\magics\\extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/magics/extension.py?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m module_str:\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/magics/extension.py?line=31'>32</a>\u001b[0m     \u001b[39mraise\u001b[39;00m UsageError(\u001b[39m'\u001b[39m\u001b[39mMissing module name.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/magics/extension.py?line=32'>33</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell\u001b[39m.\u001b[39;49mextension_manager\u001b[39m.\u001b[39;49mload_extension(module_str)\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/magics/extension.py?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39malready loaded\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/magics/extension.py?line=35'>36</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m extension is already loaded. To reload it, use:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m module_str)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\extensions.py:76\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=68'>69</a>\u001b[0m \u001b[39m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=69'>70</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=70'>71</a>\u001b[0m \u001b[39mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=71'>72</a>\u001b[0m \u001b[39m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=72'>73</a>\u001b[0m \u001b[39mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=73'>74</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=74'>75</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_extension(module_str)\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=76'>77</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=77'>78</a>\u001b[0m     \u001b[39mif\u001b[39;00m module_str \u001b[39min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\extensions.py:92\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=89'>90</a>\u001b[0m \u001b[39mif\u001b[39;00m module_str \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=90'>91</a>\u001b[0m     \u001b[39mwith\u001b[39;00m prepended_to_syspath(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mipython_extension_dir):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=91'>92</a>\u001b[0m         mod \u001b[39m=\u001b[39m import_module(module_str)\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=92'>93</a>\u001b[0m         \u001b[39mif\u001b[39;00m mod\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\u001b[39m.\u001b[39mstartswith(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mipython_extension_dir):\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=93'>94</a>\u001b[0m             \u001b[39mprint\u001b[39m((\u001b[39m\"\u001b[39m\u001b[39mLoading extensions from \u001b[39m\u001b[39m{dir}\u001b[39;00m\u001b[39m is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=94'>95</a>\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mWe recommend managing extensions like any \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=95'>96</a>\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mother Python packages, in site-packages.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='file:///c%3A/Users/Talon/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/IPython/core/extensions.py?line=96'>97</a>\u001b[0m                   \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mcompress_user(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mipython_extension_dir)))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3312.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Program%20Files/WindowsApps/PythonSoftwareFoundation.Python.3.9_3.9.3312.0_x64__qbz5n2kfra8p0/lib/importlib/__init__.py?line=124'>125</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Program%20Files/WindowsApps/PythonSoftwareFoundation.Python.3.9_3.9.3312.0_x64__qbz5n2kfra8p0/lib/importlib/__init__.py?line=125'>126</a>\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Program%20Files/WindowsApps/PythonSoftwareFoundation.Python.3.9_3.9.3312.0_x64__qbz5n2kfra8p0/lib/importlib/__init__.py?line=126'>127</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gvmagic'"
     ]
    }
   ],
   "source": [
    "# Create main function, call the various functions in order, such that the variables are passable to one another without being global\n",
    "from graphviz import render\n",
    "def main():\n",
    "    # These global variables define the path in which this workflow will be run, as well as the files for both reads \n",
    "    # and query inputs. \n",
    "    # These inputs default to the local directory in which this notebook is stored, along with default filenames. \n",
    "    # The Query file is expected to have a first line identifiying the file as query, if this in untrue remove the \"[1:]\"\n",
    "    #  after readlines\" on line 29.\n",
    "\n",
    "    ReadDirectory = (input(\"Enter the directory of the FASTA reads file: \")) or (os.path.join(sys.path[0]))\n",
    "    QueryDirectory = (input(\"Enter the directory of the QUERY query file: \")) or (os.path.join(sys.path[0]))\n",
    "    ReadFilename = (input(\"Enter the filename of the FASTA reads file: \")) or (\"READS.fasta\")\n",
    "    QueryFilename = (input(\"Enter the filename of the  reads file: \")) or (\"QUERY.fasta\")\n",
    "    global edge_dict\n",
    "    edge_dict = {}\n",
    "    # This converts the various user inputs into global variables for later use in the program\n",
    "    Test_FASTAreader(ReadDirectory, QueryDirectory, ReadFilename, QueryFilename) \n",
    "    Test_FASTA_to_dict((os.path.join(ReadDirectory, ReadFilename)))\n",
    "    fasta_dict_to_kv_lists(fasta_dict)\n",
    "    nodes, edges = DuhBrewin(Testing_multi_kmer, kmer_size)\n",
    "    # Use graphviz to \n",
    "    %load_ext gvmagic \n",
    "    dotfiledbgraph = rundot(graph_representation(Testing_multi_kmer, kmer_size))\n",
    "    Real_sequences_for_testing = sequences_from_fasta[0:1]\n",
    "    fixed_seqs = [item for elem in Real_sequences_for_testing for item in elem]\n",
    "    real_nodes, real_edges = DuhBrewin(fixed_seqs, 4)\n",
    "    pairs_to_dict(real_edges, edge_dict)\n",
    "    print(\"This is where the edge dictionary should be for real edges \")\n",
    "    print(edge_dict)\n",
    "    print(\"This should be right before the error\" )\n",
    "    print('')\n",
    "    contig_found = visualize_Eulerian_tour(real_edges)\n",
    "    # This is the output of the the contig building function, need to return and assign it to a variable \n",
    "    #print(contig_found)\n",
    "    #print(len(contig_found))\n",
    "    p = \"TGGCTA\"\n",
    "    p_bm = BoyerMoore(p)\n",
    "    print(boyer_moore(p, p_bm, contig_found))\n",
    "    print(contig_found)\n",
    "    print(contig_found[13-1:(14+(len(p)))])\n",
    "    print(p)\n",
    "    # print(dotfiledbgraph)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Quick sanity check for testing, will be made into a fully testable aspect of the code later \n",
    "\n",
    "    #print(query_Index(test_p_for_indexing_function, test_t_for_indexing_function, index))\n",
    "\n",
    "# One final double check to find out whether that supposed match is a complete match by printing the query string from the starting index\n",
    "# to the length of the pattern being used to search it. If they do not match, do not proceed. \n",
    "#    test_t_for_indexing_function = \"GCTACGATCTAGAATCTA\"\n",
    "#    test_p_for_indexing_function = \"TCTA\"\n",
    "#    print(test_t_for_indexing_function[7:(7 + len(test_p_for_indexing_function))])\n",
    "#    Sanity_check_for_indexer = (test_t_for_indexing_function[7:(7 + len(test_p_for_indexing_function))])\n",
    "#    print(Sanity_check_for_indexer)\n",
    "#    print(test_p_for_indexing_function)\n",
    "#    if Sanity_check_for_indexer != test_p_for_indexing_function:\n",
    "#        print(\"The indexing functtion is malfunctioning.\")\n",
    "#    else:\n",
    "#        print(\"Look mom, we did it!\")\n",
    "#    p = 'TCAA'\n",
    "#    p_bm = BoyerMoore(p)\n",
    "#    p_bm.bad_character_rule(2, 'T')\n",
    "#\n",
    "# Callin main function, which calls all the other functions, the driver of the program. \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section does the following:\n",
    "Imports necessary modules and packages\n",
    "# The following two actions have defaults upon no user entry based on the location of the script.\n",
    "Takes user inputs for the directory of FASTA file to be assembled\n",
    "Takes user input for the directory of a query file \n",
    "# The following two actions have defaults upon no user entry, filenames default to those included in the repository\n",
    "Takes user input for the filename of a FASTA file to be assembled\n",
    "Takes user input for the filename of a query file to be used to find matches within the assembled contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def z_array(s):\n",
    "    \"\"\" Use Z algorithm (Gusfield theorem 1.4.1) to preprocess s \"\"\"\n",
    "    assert len(s) > 1\n",
    "    z = [len(s)] + [0] * (len(s)-1)\n",
    "    # Initial comparison of s[1:] with prefix\n",
    "    for i in range(1, len(s)):\n",
    "        if s[i] == s[i-1]:\n",
    "            z[1] += 1\n",
    "        else:\n",
    "            break\n",
    "    r, l = 0, 0\n",
    "    if z[1] > 0:\n",
    "        r, l = z[1], 1\n",
    "    for k in range(2, len(s)):\n",
    "        assert z[k] == 0\n",
    "        if k > r:\n",
    "            # Case 1\n",
    "            for i in range(k, len(s)):\n",
    "                if s[i] == s[i-k]:\n",
    "                    z[k] += 1\n",
    "                else:\n",
    "                    break\n",
    "            r, l = k + z[k] - 1, k\n",
    "        else:\n",
    "            # Case 2\n",
    "            # Calculate length of beta\n",
    "            nbeta = r - k + 1\n",
    "            zkp = z[k - l]\n",
    "            if nbeta > zkp:\n",
    "                # Case 2a: Zkp wins\n",
    "                z[k] = zkp\n",
    "            else:\n",
    "                # Case 2b: Compare characters just past r\n",
    "                nmatch = 0\n",
    "                for i in range(r+1, len(s)):\n",
    "                    if s[i] == s[i - k]:\n",
    "                        nmatch += 1\n",
    "                    else:\n",
    "                        break\n",
    "                l, r = k, r + nmatch\n",
    "                z[k] = r - k + 1\n",
    "    return z\n",
    "\n",
    "\n",
    "def n_array(s):\n",
    "    \"\"\" Compile the N array (Gusfield theorem 2.2.2) from the Z array \"\"\"\n",
    "    return z_array(s[::-1])[::-1]\n",
    "\n",
    "\n",
    "def big_l_prime_array(p, n):\n",
    "    \"\"\" Compile L' array (Gusfield theorem 2.2.2) using p and N array.\n",
    "        L'[i] = largest index j less than n such that N[j] = |P[i:]| \"\"\"\n",
    "    lp = [0] * len(p)\n",
    "    for j in range(len(p)-1):\n",
    "        i = len(p) - n[j]\n",
    "        if i < len(p):\n",
    "            lp[i] = j + 1\n",
    "    return lp\n",
    "\n",
    "\n",
    "def big_l_array(p, lp):\n",
    "    \"\"\" Compile L array (Gusfield theorem 2.2.2) using p and L' array.\n",
    "        L[i] = largest index j less than n such that N[j] >= |P[i:]| \"\"\"\n",
    "    l = [0] * len(p)\n",
    "    l[1] = lp[1]\n",
    "    for i in range(2, len(p)):\n",
    "        l[i] = max(l[i-1], lp[i])\n",
    "    return l\n",
    "\n",
    "\n",
    "def small_l_prime_array(n):\n",
    "    \"\"\" Compile lp' array (Gusfield theorem 2.2.4) using N array. \"\"\"\n",
    "    small_lp = [0] * len(n)\n",
    "    for i in range(len(n)):\n",
    "        if n[i] == i+1:  # prefix matching a suffix\n",
    "            small_lp[len(n)-i-1] = i+1\n",
    "    for i in range(len(n)-2, -1, -1):  # \"smear\" them out to the left\n",
    "        if small_lp[i] == 0:\n",
    "            small_lp[i] = small_lp[i+1]\n",
    "    return small_lp\n",
    "\n",
    "\n",
    "def good_suffix_table(p):\n",
    "    \"\"\" Return tables needed to apply good suffix rule. \"\"\"\n",
    "    n = n_array(p)\n",
    "    lp = big_l_prime_array(p, n)\n",
    "    return lp, big_l_array(p, lp), small_l_prime_array(n)\n",
    "\n",
    "\n",
    "def good_suffix_mismatch(i, big_l_prime, small_l_prime):\n",
    "    \"\"\" Given a mismatch at offset i, and given L/L' and l' arrays,\n",
    "        return amount to shift as determined by good suffix rule. \"\"\"\n",
    "    length = len(big_l_prime)\n",
    "    assert i < length\n",
    "    if i == length - 1:\n",
    "        return 0\n",
    "    i += 1  # i points to leftmost matching position of P\n",
    "    if big_l_prime[i] > 0:\n",
    "        return length - big_l_prime[i]\n",
    "    return length - small_l_prime[i]\n",
    "\n",
    "\n",
    "def good_suffix_match(small_l_prime):\n",
    "    \"\"\" Given a full match of P to T, return amount to shift as\n",
    "        determined by good suffix rule. \"\"\"\n",
    "    return len(small_l_prime) - small_l_prime[1]\n",
    "\n",
    "\n",
    "def dense_bad_char_tab(p, amap):\n",
    "    \"\"\" Given pattern string and list with ordered alphabet characters, create\n",
    "        and return a dense bad character table.  Table is indexed by offset\n",
    "        then by character. \"\"\"\n",
    "    tab = []\n",
    "    nxt = [0] * len(amap)\n",
    "    for i in range(0, len(p)):\n",
    "        c = p[i]\n",
    "        assert c in amap\n",
    "        tab.append(nxt[:])\n",
    "        nxt[amap[c]] = i+1\n",
    "    return tab\n",
    "\n",
    "\n",
    "class BoyerMoore(object):\n",
    "    \"\"\" Encapsulates pattern and associated Boyer-Moore preprocessing. \"\"\"\n",
    "    \n",
    "    def __init__(self, p, alphabet='ACGT'):\n",
    "        self.p = p\n",
    "        self.alphabet = alphabet\n",
    "        # Create map from alphabet characters to integers\n",
    "        self.amap = {}\n",
    "        for i in range(len(self.alphabet)):\n",
    "            self.amap[self.alphabet[i]] = i\n",
    "        # Make bad character rule table\n",
    "        self.bad_char = dense_bad_char_tab(p, self.amap)\n",
    "        # Create good suffix rule table\n",
    "        _, self.big_l, self.small_l_prime = good_suffix_table(p)\n",
    "    \n",
    "    def bad_character_rule(self, i, c):\n",
    "        \"\"\" Return # skips given by bad character rule at offset i \"\"\"\n",
    "        assert c in self.amap\n",
    "        ci = self.amap[c]\n",
    "        assert i > (self.bad_char[i][ci]-1)\n",
    "        return i - (self.bad_char[i][ci]-1)\n",
    "    \n",
    "    def good_suffix_rule(self, i):\n",
    "        \"\"\" Given a mismatch at offset i, return amount to shift\n",
    "            as determined by (weak) good suffix rule. \"\"\"\n",
    "        length = len(self.big_l)\n",
    "        assert i < length\n",
    "        if i == length - 1:\n",
    "            return 0\n",
    "        i += 1  # i points to leftmost matching position of P\n",
    "        if self.big_l[i] > 0:\n",
    "            return length - self.big_l[i]\n",
    "        return length - self.small_l_prime[i]\n",
    "    \n",
    "    def match_skip(self):\n",
    "        \"\"\" Return amount to shift in case where P matches T \"\"\"\n",
    "        return len(self.small_l_prime) - self.small_l_prime[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sequences_from_test = ['TATTGTT', 'GCATGG', 'GCATGCA', 'TGGCTC', 'ATTGTTTTTAGA']\n",
    "#testing_nodes, testing_edges = DuhBrewin(sequences_from_test, 4)\n",
    "#print(testing_nodes)\n",
    "#print(\"\")\n",
    "#print(\"And edges\")\n",
    "#print(testing_edges)\n",
    "FASTAquerysmall = \"GGGATCGGCCATTGAACAAGATGGATTGCACGCAGGTTCT\"\n",
    "contig_found = \"AAAACTTCGAGGGATCGGCCATTGAACAAGATGGATTGCACGCAGGTTCTAATAC\"\n",
    "p_bm = BoyerMoore(FASTAquery)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
